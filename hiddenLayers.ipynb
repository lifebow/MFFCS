{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1580f9f1",
   "metadata": {},
   "source": [
    "## 1. Giới thiệu \n",
    "Multilayer Perceptron (MLP) là một mô hình mạng nơ-ron nhân tạo có cấu trúc gồm ít nhất ba lớp: một lớp đầu vào, một hoặc nhiều lớp ẩn, và một lớp đầu ra. Mỗi lớp bao gồm một tập hợp các nơ-ron (đơn vị tính toán), trong đó mỗi nơ-ron ở một lớp được kết nối đầy đủ với tất cả các nơ-ron ở lớp kế tiếp. MLP sử dụng hàm kích hoạt phi tuyến (như ReLU, sigmoid hoặc tanh) để tăng khả năng mô hình hóa các mối quan hệ phi tuyến giữa đầu vào và đầu ra. Mô hình này thường được huấn luyện bằng thuật toán lan truyền ngược (backpropagation) kết hợp với gradient descent để điều chỉnh trọng số các kết nối."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c41c0b0",
   "metadata": {},
   "source": [
    "## 2. Ứng dụng của Multilayer Perceptron\n",
    "\n",
    "Multilayer Perceptron (MLP) là một trong những kiến trúc cơ bản và phổ biến nhất của mạng nơ-ron nhân tạo. Nhờ cấu trúc nhiều lớp và khả năng học phi tuyến thông qua các hàm kích hoạt, MLP có thể mô hình hóa được các mối quan hệ phức tạp giữa đầu vào và đầu ra. MLP đã được ứng dụng rộng rãi trong nhiều lĩnh vực, bao gồm:\n",
    "#### Phân loại (Classification)\n",
    "MLP thường được sử dụng trong các bài toán phân loại dữ liệu như:<br>\n",
    "* Nhận diện chữ viết tay (ví dụ: phân loại chữ số MNIST)<br>\n",
    "* Phân loại email spam<br>\n",
    "* Phân loại ảnh đơn giản hoặc tín hiệu sinh học (ECG, EEG)<br>\n",
    "\n",
    "Với khả năng học từ dữ liệu đầu vào phi tuyến, MLP có thể phân chia không gian đặc trưng thành các vùng phức tạp hơn so với các mô hình tuyến tính.\n",
    "\n",
    "#### Dự đoán hồi quy (Regression)\n",
    "MLP còn được sử dụng cho các bài toán dự đoán giá trị liên tục như:<br>\n",
    "* Dự báo giá cổ phiếu, giá nhà<br>\n",
    "* Dự đoán mức tiêu thụ năng lượng<br>\n",
    "* Dự báo thời tiết<br>\n",
    "\n",
    "Nhờ cấu trúc sâu, MLP có thể học các hàm ánh xạ phức tạp từ đầu vào đến giá trị dự đoán.\n",
    "\n",
    "#### Xử lý ngôn ngữ tự nhiên (NLP)\n",
    "Trong các mô hình NLP đơn giản, MLP có thể được dùng sau các lớp biểu diễn (embedding) để thực hiện các tác vụ:\n",
    "\n",
    "* Phân loại cảm xúc (sentiment analysis)\n",
    "* Phân loại văn bản\n",
    "* Dự đoán từ tiếp theo (next word prediction)\n",
    "\n",
    "#### Thị giác máy tính (Computer Vision)\n",
    "Mặc dù ngày nay CNN (Convolutional Neural Network) được ưu tiên hơn trong thị giác máy tính, nhưng MLP vẫn được dùng cho các tác vụ đơn giản hoặc làm lớp cuối cùng (classifier) trong mô hình thị giác:\n",
    "\n",
    "* Phân loại ảnh kích thước nhỏ\n",
    "* Nhận diện đối tượng sau khi trích xuất đặc trưng"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1d23ff",
   "metadata": {},
   "source": [
    "## 3. Giải quyết vấn đề trong Computer Science\n",
    "Multilayer Perceptron (MLP) là một mô hình mạng nơ-ron truyền thống có cấu trúc nhiều lớp fully-connected, được sử dụng rộng rãi trong nhiều bài toán trong ngành Khoa học Máy tính. Nhờ khả năng học các hàm phi tuyến phức tạp, MLP đã chứng minh được hiệu quả trong nhiều lĩnh vực cốt lõi như:\n",
    "\n",
    "#### Trí tuệ nhân tạo (Artificial Intelligence)\n",
    "* Phân loại ảnh, âm thanh, văn bản: MLP có thể được huấn luyện để thực hiện các tác vụ nhận dạng mẫu trong dữ liệu đầu vào.\n",
    "* Chơi game: Trong các mô hình học tăng cường (reinforcement learning), MLP thường đóng vai trò là hàm chính sách hoặc hàm giá trị.\n",
    "#### Học máy (Machine Learning)\n",
    "* Hồi quy phi tuyến: MLP được dùng để ánh xạ đầu vào sang đầu ra phi tuyến trong các bài toán dự đoán.\n",
    "* Giảm chiều dữ liệu: Dùng trong autoencoder để biểu diễn dữ liệu ở không gian đặc trưng thấp hơn.\n",
    "#### Xử lý ngôn ngữ tự nhiên (NLP)\n",
    "* Phân loại văn bản: Như phát hiện spam, phân tích cảm xúc, hoặc nhận dạng thực thể.\n",
    "* Gán nhãn chuỗi: Dùng MLP trong các mô hình như BiLSTM+MLP để gán nhãn từ theo ngữ cảnh.\n",
    "#### Thị giác máy tính (Computer Vision)\n",
    "* Lớp phân loại sau CNN: Trong các mô hình thị giác sâu, MLP thường được dùng ở phần cuối mạng để ánh xạ đặc trưng thành nhãn đầu ra.\n",
    "* Nhận dạng mẫu đơn giản: Cho các bài toán ảnh nhỏ hoặc dữ liệu có số chiều thấp.\n",
    "#### An ninh mạng và bảo mật\n",
    "* Phát hiện xâm nhập (Intrusion Detection): MLP có thể học để phân biệt hành vi mạng bình thường và bất thường.\n",
    "* Phân tích mã độc (Malware Detection): Học từ đặc trưng của tập tin để xác định phần mềm độc hại.\n",
    "#### Khoa học dữ liệu và khai phá dữ liệu\n",
    "* Phân cụm có giám sát (Supervised Clustering)\n",
    "* Dự báo chuỗi thời gian\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d5e07d",
   "metadata": {},
   "source": [
    "## 4. Hidden Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12c5c93",
   "metadata": {},
   "source": [
    "### 4.1 Giới hạn của mô hình tuyến tính (linear model)\n",
    "Mô hình tuyến tính, mặc dù đơn giản và dễ diễn giải, lại gặp nhiều hạn chế khi áp dụng vào các bài toán phức tạp. Một trong những giả định yếu hơn mà mô hình tuyến tính chấp nhận là **tính đơn điệu (_monotonicity_)** – tức là, khi một đặc trưng tăng thì đầu ra của mô hình cũng phải tăng (nếu trọng số dương), hoặc giảm (nếu trọng số âm).  \n",
    "\n",
    "- Trong một số trường hợp, giả định này có vẻ hợp lý, ví dụ như khi dự đoán khả năng trả nợ của một cá nhân dựa vào thu nhập. Tuy nhiên, mối quan hệ này thường **không tuyến tính** — ví dụ, sự gia tăng thu nhập từ 0 lên 50.000 USD có thể ảnh hưởng nhiều hơn đến khả năng trả nợ so với việc tăng từ 1 triệu lên 1.05 triệu USD.\n",
    "\n",
    "- Trong các trường hợp khác, giả định đơn điệu thậm chí **không còn đúng nữa**. Ví dụ: dự đoán rủi ro sức khỏe từ thân nhiệt – cả khi nhiệt độ cao hơn hoặc thấp hơn 37°C đều có thể tăng rủi ro. Trong trường hợp đó, cần **tiền xử lý dữ liệu**, như lấy giá trị chênh lệch so với 37°C.\n",
    "\n",
    "- Với các bài toán như **phân loại hình ảnh mèo và chó**, mô hình tuyến tính tỏ ra không hiệu quả. Việc giả định rằng độ sáng của một điểm ảnh có thể quyết định loài vật là không hợp lý. Mô hình tuyến tính thất bại khi **đảo màu ảnh** nhưng nhãn lớp không đổi.\n",
    "\n",
    "Đây là lúc **mạng nơ-ron sâu phi tuyến** như MLP phát huy tác dụng. Các lớp ẩn và hàm kích hoạt phi tuyến giúp mạng học sâu **tự động học biểu diễn đặc trưng** và mô hình hóa các quan hệ phi tuyến mà không cần thiết kế thủ công.\n",
    "\n",
    "### 4.2 Tính phi tuyến trong lịch sử\n",
    "\n",
    "Khả năng mô hình hóa phi tuyến đã được nghiên cứu từ lâu:\n",
    "\n",
    "- **Cây quyết định**: được đề xuất bởi *Fisher (1925)*, và phát triển bởi *Quinlan (1993)*.\n",
    "- **Hàm kernel**: được sử dụng từ rất sớm trong các phương pháp spline và nonparametric (Aronszajn, 1950; Wahba, 1990).\n",
    "- **Mạng nơ-ron sinh học**: theo mô hình não bộ (Schölkopf & Smola, 2002; Ramón y Cajal, 1894).\n",
    "\n",
    "Nhờ sự kết nối nhiều tầng đơn giản, ta có thể tạo ra hệ thống học có năng lực biểu diễn mạnh, là nền tảng của học sâu hiện đại.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb09f3e",
   "metadata": {},
   "source": [
    "### 4.3 Kết hợp các lớp ẩn (hidden layers)\n",
    "\n",
    "Trong phần này, chúng ta sẽ thảo luận về sự chuyển đổi từ mô hình tuyến tính sang phi tuyến trong Mạng Perceptron Đa tầng (MLP). Đặc biệt, chúng ta tập trung vào vai trò của hàm kích hoạt phi tuyến trong việc nâng cao khả năng biểu diễn của mô hình.\n",
    "\n",
    "#### 4.3.1 Thêm lớp ẩn vào mô hình tuyến tính không dùng Activation Function\n",
    "\n",
    "Đầu vào ma trận: $\\mathbf{X} \\in \\mathbb{R}^{n \\times d}$ biểu diễn một minibatch gồm $n$ mẫu, mỗi mẫu có $d$ đặc trưng.<br >\n",
    "Tầng ẩn:<br>\n",
    "* Weights:  $\\mathbf{W}^{(1)} \\in \\mathbb{R}^{d \\times h}$. <br>\n",
    "* Biases:   $\\mathbf{b}^{(1)} \\in \\mathbb{R}^{1 \\times h}$. <br>\n",
    "* Đầu ra tâng ẩn:  $\\mathbf{H} = \\mathbf{X} \\mathbf{W}^{(1)} + \\mathbf{b}^{(1)}$ <br>\n",
    "\n",
    "Tầng output:<br>\n",
    "* Weights:  $\\mathbf{W}^{(2)} \\in \\mathbb{R}^{h \\times q}$. <br>\n",
    "* Biases:   $\\mathbf{b}^{(2)} \\in \\mathbb{R}^{1 \\times q}$. <br>\n",
    "* Đầu ra cuối cùng:  $\\mathbf{O} = \\mathbf{H} \\mathbf{W}^{(2)} + \\mathbf{b}^{(2)}$ <br>\n",
    "\n",
    "Khi không sử dụng hàm kích hoạt phi tuyến, mô hình MLP một tầng ẩn có thể được rút gọn thành một mô hình tuyến tính đơn giản:<br>\n",
    "\n",
    "$$\n",
    "\\mathbf{O} = (\\mathbf{X} \\mathbf{W}^{(1)} + \\mathbf{b}^{(1)})\\mathbf{W}^{(2)} + \\mathbf{b}^{(2)} = \\mathbf{X} \\mathbf{W}^{(1)}\\mathbf{W}^{(2)} + \\mathbf{b}^{(1)} \\mathbf{W}^{(2)} + \\mathbf{b}^{(2)} = \\mathbf{X} \\mathbf{W} + \\mathbf{b}.\n",
    "$$\n",
    "<br>\n",
    "Hệ quả: Việc thêm các tầng ẩn không làm tăng khả năng biểu diễn của mô hình vì tổ hợp các phép biến đổi tuyến tính vẫn chỉ là một phép biến đổi tuyến tính.\n",
    "\n",
    "#### 4.3.2 Vai trò của Nonlinear Activation Function\n",
    "Hàm kích hoạt phi tuyến (ví dụ: ReLU, sigmoid, tanh) được áp dụng cho từng phần tử của đầu ra tầng ẩn:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\mathbf{H} & = \\sigma(\\mathbf{X} \\mathbf{W}^{(1)} + \\mathbf{b}^{(1)}), \\\\\n",
    "    \\mathbf{O} & = \\mathbf{H}\\mathbf{W}^{(2)} + \\mathbf{b}^{(2)}.\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Tác dụng của Activation Function:\n",
    "* Phá vỡ tính tuyến tính: Hàm kích hoạt phi tuyến ngăn việc \"gộp\" các tầng thành một phép biến đổi tuyến tính đơn giản.\n",
    "* Tăng khả năng biểu diễn: Mô hình có thể học các hàm phức tạp hơn, bao gồm các hàm phân tầng hoặc đa thức."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea552713",
   "metadata": {},
   "source": [
    "### 4.4 Hàm kích hoạt\n",
    "Hàm kích hoạt trong mạng neural là hàm phi tuyến được áp dụng lên đầu ra của mỗi lớp (thường là sau phép biến đổi tuyến tính), giúp mạng có thể học được các quan hệ phi tuyến phức tạp trong dữ liệu.\n",
    "#### 4.4.1 Hàm kích hoạt ReLU "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac448ee",
   "metadata": {},
   "source": [
    "## 5. Bài tập"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
