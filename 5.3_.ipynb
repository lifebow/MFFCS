{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lan truyền thuận, lan truyền ngược và đồ thị tính toán\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lý thuyết\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "### Problem Statement\n",
    "\n",
    "Trong học sâu (deep learning), một bài toán quan trọng là **huấn luyện mạng neural** sao cho nó có thể học được các mẫu (patterns) từ dữ liệu và tổng quát hóa tốt khi gặp dữ liệu mới. Quá trình huấn luyện này yêu cầu **điều chỉnh trọng số và bias** của mạng để **giảm sai số (loss)** giữa đầu ra mô hình và nhãn thật sự.\n",
    "\n",
    "Một vấn đề cốt lõi là: **Làm sao để tính được độ dốc (gradient) của hàm mất mát theo từng tham số trong một mạng có nhiều lớp?**  \n",
    "=> **Backpropagation** ra đời để giải quyết chính xác bài toán này.\n",
    "\n",
    "---\n",
    "\n",
    "### Applications\n",
    "\n",
    "Backpropagation là nền tảng cho hầu hết các mô hình deep learning hiện đại, như:\n",
    "\n",
    "- Nhận diện ảnh (Image Classification)\n",
    "- Nhận diện chữ viết tay (Digit Recognition - ví dụ: MNIST)\n",
    "- Xử lý ngôn ngữ tự nhiên (NLP)\n",
    "- Hệ thống gợi ý (Recommendation Systems)\n",
    "- Dự đoán chuỗi thời gian (Time Series Forecasting)\n",
    "\n",
    "---\n",
    "\n",
    "### How CS Problems are Solved using Backpropagation\n",
    "\n",
    "Trong khoa học máy tính, đặc biệt trong lĩnh vực trí tuệ nhân tạo và machine learning, backpropagation giúp giải quyết nhiều bài toán bằng cách:\n",
    "\n",
    "- Cho phép **huấn luyện mạng nhiều lớp (deep neural networks)** hiệu quả.\n",
    "- Cung cấp **đạo hàm chính xác và nhanh** thông qua Chain Rule.\n",
    "- Làm nền tảng cho các **framework học sâu** như PyTorch, TensorFlow.\n",
    "\n",
    "Nhờ backpropagation, các mô hình có thể **liên tục cải thiện hiệu năng** qua các vòng huấn luyện, và học từ dữ liệu thực tế theo cách có thể mở rộng (scalable).\n",
    "\n",
    "\n",
    "\n",
    "### 5.3.1 Lan truyền thuận\n",
    "\n",
    "### 5.3.2 Đồ thị tính toán của lan truyền thuận\n",
    "\n",
    "### 5.3.3 Lan truyền Ngược\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bài tập\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Assume that the inputs $\\mathbf{X}$ to some scalar function $f$ are $n \\times m$ matrices. What is the dimensionality of the gradient of $f$ with respect to $\\mathbf{X}$?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Miền gradient của hàm $f$ sẽ có miền giống với miền của ma trận X, tức là $\\mathbb{R}^{n \\times m}$\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial \\mathbf{X}} \\in \\mathbb{R}^{n \\times m}.\n",
    "$$\n",
    "\n",
    "Với mỗi phần tử của ma trận, gradient sẽ là:\n",
    "$$\n",
    "\\left[ \\frac{\\partial f}{\\partial X} \\right]_{ij} = \\frac{\\partial f}{\\partial X_{ij}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial \\mathbf{X}} =\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial f}{\\partial x_{11}} & \\frac{\\partial f}{\\partial x_{21}} & \\cdots & \\frac{\\partial f}{\\partial x_{n1}} \\\\\n",
    "\\frac{\\partial f}{\\partial x_{12}} & \\frac{\\partial f}{\\partial x_{22}} & \\cdots & \\frac{\\partial f}{\\partial x_{n2}} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\partial f}{\\partial x_{1m}} & \\frac{\\partial f}{\\partial x_{2m}} & \\cdots & \\frac{\\partial f}{\\partial x_{nm}}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Add a bias to the hidden layer of the model described in this section (you do not need to include bias in the regularization term).\n",
    "1. Draw the corresponding computational graph.\n",
    "2. Derive the forward and backward propagation equations.\n",
    "\n",
    "\n",
    "Dựa theo đồ thị tính toán ở phần lý thuyết, ta thêm bias vào hidden layer, ta có thể biểu diễn lại công thức tính toán của hidden layer như sau:\n",
    "\n",
    "Input to hidden layer:\n",
    "$$\n",
    "\\mathbf{z} = \\mathbf{W}^{(1)} \\mathbf{x} + \\mathbf{b_{1}}\n",
    "$$\n",
    "\n",
    "Hidden layer to activation:\n",
    "$$\n",
    "\\mathbf{h} = \\phi(\\mathbf{z})\n",
    "$$\n",
    "\n",
    "Output layer:\n",
    "$$\n",
    "\\mathbf{o} = \\mathbf{W}^{(2)} \\mathbf{h} + \\mathbf{b_{2}}\n",
    "$$\n",
    "\n",
    "Hàm mất mát:\n",
    "$$\n",
    "L = l(o,y)\n",
    "$$\n",
    "\n",
    "Hàm mục tiêu:\n",
    "$$\n",
    "J = L + \\frac{\\lambda}{2}(||W^{(1)}||_F^2+||W^{(2)}||_F^2)\n",
    "$$\n",
    "\n",
    "\n",
    "![Đồ thị tính toán](./images/computational%20graph.png)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Compute the memory footprint for training and prediction in the model described in this section.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Assume that you want to compute second derivatives. What happens to the computational graph? How long do you expect the calculation to take?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 5. Assume that the computational graph is too large for your GPU.\n",
    "1. Can you partition it over more than one GPU?\n",
    "2. What are the advantages and disadvantages over training on a smaller minibatch?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
